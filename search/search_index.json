{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting Started Weclome Welcome to OpenPoints library. OpenPoints is a machine learning codebase for point-based methods for point cloud understanding. The biggest difference between OpenPoints and other libraries is that we focus more on reproducibility and fair benchmarking. Extensibility : supports many representative networks for point cloud understanding, such as PointNet, DGCNN, DeepGCN, PointNet++, ASSANet , PointMLP, PointNeXt , and Pix4Point . More networks can be built easily based on our framework since OpenPoints support a wide range of basic operations including graph convolutions, linear convolutions, local aggregation modules, self-attention, farthest point sampling, ball query, *e.t.c . Reproducibility : all implemented models are trained on various tasks at least three times. Mean\u00b1std is provided in the PointNeXt paper . Pretrained models and logs are available. Fair Benchmarking : in PointNeXt, we find a large part of performance gain is due to the training strategies. In OpenPoints, all models are trained with the improved training strategies and all achieve much higher accuracy than the original reported value. Ease of Use : Build model, optimizer, scheduler, loss function, and data loader easily from cfg . Train and validate different models on various tasks by simply changing the cfg\\*\\*.yaml file. model = build_model_from_cfg(cfg.model) criterion = build_criterion_from_cfg(cfg.criterion_args) Here is an example of pointnet.yaml (model configuration for PointNet model): model : NAME : BaseCls encoder_args : NAME : PointNetEncoder in_channels : 4 cls_args : NAME : ClsHead num_classes : 15 in_channels : 1024 mlps : [ 512 , 256 ] norm_args : norm : 'bn1d' Online logging : Support wandb for checking your results anytime anywhere. Install git clone git@github.com:guochengqian/PointNeXt.git cd PointNeXt source install.sh Note: 1) the install.sh requires CUDA 11.3; if another version of CUDA is used, install.sh has to be modified accordingly; check your CUDA version by: nvcc --version before using the bash file; 2) you might need to read install.sh for a step-by-step installation if the bash file ( install.sh ) does not work for you by any chance; 3) for all experiments, we use wandb for online logging. Run wandb --login only at the first time in a new machine. Set wandn.use_wandb=False to use this function. Read the official wandb documentation if needed. General Usage All experiments follow the simple rule to train and test (run in the root directory): CUDA_VISIBLE_DEVICES=$GPUs python examples/$task_folder/main.py --cfg $cfg $kwargs $GPUs is the list of GPUs to use, for most experiments (ScanObjectNN, ModelNet40, S3DIS), we only use 1 A100 (GPUs=0) $task_folder is the folder name of the experiment. For example, for s3dis segmentation, $task_folder=s3dis $cfg is the path to cfg, for example, s3dis segmentation, $cfg=cfgs/s3dis/pointnext-s.yaml $kwargs is used to overwrite the default configs. E.g. overwrite the batch size, just appending batch_size=32 or --batch_size 32 . As another example, testing in S3DIS area 5, $kwargs should be mode=test --pretrained_path $pretrained_path .","title":"Getting Started"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#weclome","text":"Welcome to OpenPoints library. OpenPoints is a machine learning codebase for point-based methods for point cloud understanding. The biggest difference between OpenPoints and other libraries is that we focus more on reproducibility and fair benchmarking. Extensibility : supports many representative networks for point cloud understanding, such as PointNet, DGCNN, DeepGCN, PointNet++, ASSANet , PointMLP, PointNeXt , and Pix4Point . More networks can be built easily based on our framework since OpenPoints support a wide range of basic operations including graph convolutions, linear convolutions, local aggregation modules, self-attention, farthest point sampling, ball query, *e.t.c . Reproducibility : all implemented models are trained on various tasks at least three times. Mean\u00b1std is provided in the PointNeXt paper . Pretrained models and logs are available. Fair Benchmarking : in PointNeXt, we find a large part of performance gain is due to the training strategies. In OpenPoints, all models are trained with the improved training strategies and all achieve much higher accuracy than the original reported value. Ease of Use : Build model, optimizer, scheduler, loss function, and data loader easily from cfg . Train and validate different models on various tasks by simply changing the cfg\\*\\*.yaml file. model = build_model_from_cfg(cfg.model) criterion = build_criterion_from_cfg(cfg.criterion_args) Here is an example of pointnet.yaml (model configuration for PointNet model): model : NAME : BaseCls encoder_args : NAME : PointNetEncoder in_channels : 4 cls_args : NAME : ClsHead num_classes : 15 in_channels : 1024 mlps : [ 512 , 256 ] norm_args : norm : 'bn1d' Online logging : Support wandb for checking your results anytime anywhere.","title":"Weclome"},{"location":"#install","text":"git clone git@github.com:guochengqian/PointNeXt.git cd PointNeXt source install.sh Note: 1) the install.sh requires CUDA 11.3; if another version of CUDA is used, install.sh has to be modified accordingly; check your CUDA version by: nvcc --version before using the bash file; 2) you might need to read install.sh for a step-by-step installation if the bash file ( install.sh ) does not work for you by any chance; 3) for all experiments, we use wandb for online logging. Run wandb --login only at the first time in a new machine. Set wandn.use_wandb=False to use this function. Read the official wandb documentation if needed.","title":"Install"},{"location":"#general-usage","text":"All experiments follow the simple rule to train and test (run in the root directory): CUDA_VISIBLE_DEVICES=$GPUs python examples/$task_folder/main.py --cfg $cfg $kwargs $GPUs is the list of GPUs to use, for most experiments (ScanObjectNN, ModelNet40, S3DIS), we only use 1 A100 (GPUs=0) $task_folder is the folder name of the experiment. For example, for s3dis segmentation, $task_folder=s3dis $cfg is the path to cfg, for example, s3dis segmentation, $cfg=cfgs/s3dis/pointnext-s.yaml $kwargs is used to overwrite the default configs. E.g. overwrite the batch size, just appending batch_size=32 or --batch_size 32 . As another example, testing in S3DIS area 5, $kwargs should be mode=test --pretrained_path $pretrained_path .","title":"General Usage"},{"location":"changes/","text":"Change Logs & TODOs Change Logs [2022/08/21] Added ShapeNetPart and ScanNet. Support AMP. [2022/07/09] Initial Realse TODO clean segmentation support multi-gpu testing for segmentation tasks","title":"Change Logs & TODOs"},{"location":"changes/#change-logs-todos","text":"","title":"Change Logs &amp; TODOs"},{"location":"changes/#change-logs","text":"[2022/08/21] Added ShapeNetPart and ScanNet. Support AMP. [2022/07/09] Initial Realse","title":"Change Logs"},{"location":"changes/#todo","text":"clean segmentation support multi-gpu testing for segmentation tasks","title":"TODO"},{"location":"modelzoo/","text":"Model Zoo (Pretrained Models) We provide the training logs & pretrained models in column our released trained with the improved training strategies proposed by our PointNeXt through Google Drive. TP : Throughput (instance per second) measured using an NVIDIA Tesla V100 32GB GPU and a 32 core Intel Xeon @ 2.80GHz CPU. ScanObjectNN (Hardest variant) Classification Throughput is measured with 128 x 1024 points. name OA/mAcc (Original) OA/mAcc (our released) #params FLOPs Throughput (ins./sec.) PointNet 68.2 / 63.4 75.2 / 71.4 3.5M 1.0G 4212 DGCNN 78.1 / 73.6 86.1 / 84.3 1.8M 4.8G 402 PointMLP 85.4\u00b11.3 / 83.9\u00b11.5 87.7 / 86.4 13.2M 31.4G 191 PointNet++ 77.9 / 75.4 86.2 / 84.4 1.5M 1.7G 1872 PointNeXt-S 87.7\u00b10.4 / 85.8\u00b10.6 88.20 / 86.84 1.4M 1.64G 2040 S3IDS (6-fold) Segmentation Throughput (TP) is measured with 16 x 15000 points. name mIoU/OA/mAcc (Original) mIoU/OA/mAcc (our released) #params FLOPs TP PointNet++ 54.5 / 81.0 / 67.1 68.1 / 87.6 / 78.4 1.0M 7.2G 186 PointNeXt-S 68.0 / 87.4 / 77.3 68.0 / 87.4 / 77.3 0.8M 3.6G 227 PointNeXt-B 71.5 / 88.8 / 80.2 71.5 / 88.8 / 80.2 3.8M 8.8G 158 PointNeXt-L 73.9 / 89.8 / 82.2 73.9 / 89.8 / 82.2 7.1M 15.2G 115 PointNeXt-XL 74.9 / 90.3 / 83.0 74.9 / 90.3 / 83.0 41.6M 84.8G 46 S3DIS (Area 5) Segmentation Throughput (TP) is measured with 16 x 15000 points. name mIoU/OA/mAcc (Original) mIoU/OA/mAcc (our released) #params FLOPs TP PointNet++ 53.5 / 83.0 / - 63.6 / 88.3 / 70.2 1.0M 7.2G 186 ASSANet 63.0 / - /- 65.8 / 88.9 / 72.2 2.4M 2.5G 228 ASSANet-L 66.8 / - / - 68.0 / 89.7/ 74.3 115.6M 36.2G 81 PointNeXt-S 63.4\u00b10.8 / 87.9\u00b10.3 / 70.0\u00b10.7 64.2 / 88.2 / 70.7 0.8M 3.6G 227 PointNeXt-B 67.3\u00b10.2 / 89.4\u00b10.1 / 73.7\u00b10.6 67.5 / 89.4 / 73.9 3.8M 8.8G 158 PointNeXt-L 69.0\u00b10.5 / 90.0\u00b10.1 / 75.3\u00b10.8 69.3 / 90.1 / 75.7 7.1M 15.2G 115 PointNeXt-XL 70.5\u00b10.3 / 90.6\u00b10.2 / 76.8\u00b10.7 71.1 / 91.0 / 77.2 41.6M 84.8G 46 ShapeNetpart Part Segmentation Throughput (TP) is measured with 64*2048 points. name Ins. mIoU / Cat. mIoU (Original) Ins. mIoU / Cat. mIoU (our released) PointNet++ 85.1/81.9 PointNeXt-S 86.7\u00b10.0 / 84.4\u00b10.2 86.7 / 84.2 PointNeXt-S (C=64) 86.9\u00b10.0 / 84.8\u00b10.5 86.9 / 85.2 PointNeXt-S (C=160) 87.0\u00b10.1 / 85.2\u00b10.1 87.1 / 85.4 ModelNet40 Classificaiton name OA/mAcc (Original) OA/mAcc (our released) #params FLOPs Throughput (ins./sec.) PointNet++ 91.9 / - 93.0 / 90.7 1.5M 1.7G 1872 PointNeXt-S (C=64) 93.7\u00b10.3 / 90.9\u00b10.5 94.0 / 91.1 4.5M 6.5G 2033","title":"Model Zoo (Pretrained Models)"},{"location":"modelzoo/#model-zoo-pretrained-models","text":"We provide the training logs & pretrained models in column our released trained with the improved training strategies proposed by our PointNeXt through Google Drive. TP : Throughput (instance per second) measured using an NVIDIA Tesla V100 32GB GPU and a 32 core Intel Xeon @ 2.80GHz CPU.","title":"Model Zoo (Pretrained Models)"},{"location":"modelzoo/#scanobjectnn-hardest-variant-classification","text":"Throughput is measured with 128 x 1024 points. name OA/mAcc (Original) OA/mAcc (our released) #params FLOPs Throughput (ins./sec.) PointNet 68.2 / 63.4 75.2 / 71.4 3.5M 1.0G 4212 DGCNN 78.1 / 73.6 86.1 / 84.3 1.8M 4.8G 402 PointMLP 85.4\u00b11.3 / 83.9\u00b11.5 87.7 / 86.4 13.2M 31.4G 191 PointNet++ 77.9 / 75.4 86.2 / 84.4 1.5M 1.7G 1872 PointNeXt-S 87.7\u00b10.4 / 85.8\u00b10.6 88.20 / 86.84 1.4M 1.64G 2040","title":"ScanObjectNN (Hardest variant) Classification"},{"location":"modelzoo/#s3ids-6-fold-segmentation","text":"Throughput (TP) is measured with 16 x 15000 points. name mIoU/OA/mAcc (Original) mIoU/OA/mAcc (our released) #params FLOPs TP PointNet++ 54.5 / 81.0 / 67.1 68.1 / 87.6 / 78.4 1.0M 7.2G 186 PointNeXt-S 68.0 / 87.4 / 77.3 68.0 / 87.4 / 77.3 0.8M 3.6G 227 PointNeXt-B 71.5 / 88.8 / 80.2 71.5 / 88.8 / 80.2 3.8M 8.8G 158 PointNeXt-L 73.9 / 89.8 / 82.2 73.9 / 89.8 / 82.2 7.1M 15.2G 115 PointNeXt-XL 74.9 / 90.3 / 83.0 74.9 / 90.3 / 83.0 41.6M 84.8G 46","title":"S3IDS (6-fold) Segmentation"},{"location":"modelzoo/#s3dis-area-5-segmentation","text":"Throughput (TP) is measured with 16 x 15000 points. name mIoU/OA/mAcc (Original) mIoU/OA/mAcc (our released) #params FLOPs TP PointNet++ 53.5 / 83.0 / - 63.6 / 88.3 / 70.2 1.0M 7.2G 186 ASSANet 63.0 / - /- 65.8 / 88.9 / 72.2 2.4M 2.5G 228 ASSANet-L 66.8 / - / - 68.0 / 89.7/ 74.3 115.6M 36.2G 81 PointNeXt-S 63.4\u00b10.8 / 87.9\u00b10.3 / 70.0\u00b10.7 64.2 / 88.2 / 70.7 0.8M 3.6G 227 PointNeXt-B 67.3\u00b10.2 / 89.4\u00b10.1 / 73.7\u00b10.6 67.5 / 89.4 / 73.9 3.8M 8.8G 158 PointNeXt-L 69.0\u00b10.5 / 90.0\u00b10.1 / 75.3\u00b10.8 69.3 / 90.1 / 75.7 7.1M 15.2G 115 PointNeXt-XL 70.5\u00b10.3 / 90.6\u00b10.2 / 76.8\u00b10.7 71.1 / 91.0 / 77.2 41.6M 84.8G 46","title":"S3DIS (Area 5) Segmentation"},{"location":"modelzoo/#shapenetpart-part-segmentation","text":"Throughput (TP) is measured with 64*2048 points. name Ins. mIoU / Cat. mIoU (Original) Ins. mIoU / Cat. mIoU (our released) PointNet++ 85.1/81.9 PointNeXt-S 86.7\u00b10.0 / 84.4\u00b10.2 86.7 / 84.2 PointNeXt-S (C=64) 86.9\u00b10.0 / 84.8\u00b10.5 86.9 / 85.2 PointNeXt-S (C=160) 87.0\u00b10.1 / 85.2\u00b10.1 87.1 / 85.4","title":"ShapeNetpart Part Segmentation"},{"location":"modelzoo/#modelnet40-classificaiton","text":"name OA/mAcc (Original) OA/mAcc (our released) #params FLOPs Throughput (ins./sec.) PointNet++ 91.9 / - 93.0 / 90.7 1.5M 1.7G 1872 PointNeXt-S (C=64) 93.7\u00b10.3 / 90.9\u00b10.5 94.0 / 91.1 4.5M 6.5G 2033","title":"ModelNet40 Classificaiton"},{"location":"examples/modelnet/","text":"Point cloud classification on ModelNet40 Note in this experiment, we do not use any re-sampled version of ModelNet40 (more than 2K points) or any normal information. The data we use is: modelnet40_ply_hdf5_2048 [1]. Dataset ModelNet40 dataset will be downloaded automatically. Train For example, train PointNeXt-S CUDA_VISIBLE_DEVICES = 1 python examples/classification/main.py --cfg cfgs/modelnet40ply2048/pointnext-s.yaml Test test PointNeXt-S (C=64) CUDA_VISIBLE_DEVICES = 1 python examples/classification/main.py --cfg cfgs/modelnet40ply2048/pointnext-s.yaml model.encoder_args.width = 64 mode = test --pretrained_path /path/to/your/pretrained_model Reference @InProceedings{wu2015modelnet, author = {Wu, Zhirong and Song, Shuran and Khosla, Aditya and Yu, Fisher and Zhang, Linguang and Tang, Xiaoou and Xiao, Jianxiong}, title = {3D ShapeNets: A Deep Representation for Volumetric Shapes}, booktitle = {CVPR}, year = {2015} }","title":"Point cloud classification on ModelNet40"},{"location":"examples/modelnet/#point-cloud-classification-on-modelnet40","text":"Note in this experiment, we do not use any re-sampled version of ModelNet40 (more than 2K points) or any normal information. The data we use is: modelnet40_ply_hdf5_2048 [1].","title":"Point cloud classification on ModelNet40"},{"location":"examples/modelnet/#dataset","text":"ModelNet40 dataset will be downloaded automatically.","title":"Dataset"},{"location":"examples/modelnet/#train","text":"For example, train PointNeXt-S CUDA_VISIBLE_DEVICES = 1 python examples/classification/main.py --cfg cfgs/modelnet40ply2048/pointnext-s.yaml","title":"Train"},{"location":"examples/modelnet/#test","text":"test PointNeXt-S (C=64) CUDA_VISIBLE_DEVICES = 1 python examples/classification/main.py --cfg cfgs/modelnet40ply2048/pointnext-s.yaml model.encoder_args.width = 64 mode = test --pretrained_path /path/to/your/pretrained_model","title":"Test"},{"location":"examples/modelnet/#reference","text":"@InProceedings{wu2015modelnet, author = {Wu, Zhirong and Song, Shuran and Khosla, Aditya and Yu, Fisher and Zhang, Linguang and Tang, Xiaoou and Xiao, Jianxiong}, title = {3D ShapeNets: A Deep Representation for Volumetric Shapes}, booktitle = {CVPR}, year = {2015} }","title":"Reference"},{"location":"examples/s3dis/","text":"Indoor Point cloud Segmentation on S3DIS The models are trained on the subsampled point clouds (voxel size = 0.04). The model achieving the best performance on validation is selected to test on the original point clouds (not downsampled). Dataset Please cite the S3DIS paper [1] if you are going to use our presampled datasets. The presampling is just to collect all point clouds, area by area and room by room, following PointGroup . mkdir -p data/S3DIS/ cd data/S3DIS gdown https://drive.google.com/uc?id = 1MX3ZCnwqyRztG1vFRiHkKTz68ZJeHS4Y tar -xvf s3disfull.tar Organize the dataset as follows: data |--- S3DIS |--- s3disfull |--- raw |--- Area_6_pantry_1.npy |--- ... |--- processed |--- s3dis_val_area5_0.040.pkl Train For example, train PointNext-XL CUDA_VISIBLE_DEVICES = 1 python examples/segmentation/main.py cfgs/s3dis/pointnext-xl.yaml * change the cfg file to use any other model, e.g. cfgs/s3dis/pointnet++.yaml for training PointNet++ * run the command at the root directory Test on Area 5 Note testing is a must step since evaluation in training is performed only on subsampled point clouds not original point clouds. CUDA_VISIBLE_DEVICES = 1 bash script/main_segmentation.sh cfgs/s3dis/pointnext-xl.yaml wandb.use_wandb = False mode = test --pretrained_path pretrained/s3dis/pointnext-xl/pointnext-xl-area5/checkpoint/pointnext-xl_ckpt_best.pth * add visualize=True to save segmentation results as .obj files Test on All Areas CUDA_VISIBLE_DEVICES = 1 python examples/segmentation/test_6fold.py cfgs/s3dis/pointnext-xl.yaml mode = test --pretrained_path pretrained/s3dis/pointnext-xl Profile Parameters, FLOPs, and Throughput CUDA_VISIBLE_DEVICES = 1 python examples/profile.py --cfg cfgs/s3dis/pointnext-xl.yaml batch_size = 16 num_points = 15000 timing = True note: 1. set --cfg to cfgs/s3dis to profile all models under the folder. 2. you have to install the latest version of DeepSpeed from source to get a correct measurement of FLOPs Reference @inproceedings{armeni2016s3dis, title={3d semantic parsing of large-scale indoor spaces}, author={Armeni, Iro and Sener, Ozan and Zamir, Amir R and Jiang, Helen and Brilakis, Ioannis and Fischer, Martin and Savarese, Silvio}, booktitle=CVPR, pages={1534--1543}, year={2016} }","title":"Indoor Point cloud Segmentation on S3DIS"},{"location":"examples/s3dis/#indoor-point-cloud-segmentation-on-s3dis","text":"The models are trained on the subsampled point clouds (voxel size = 0.04). The model achieving the best performance on validation is selected to test on the original point clouds (not downsampled).","title":"Indoor Point cloud Segmentation on S3DIS"},{"location":"examples/s3dis/#dataset","text":"Please cite the S3DIS paper [1] if you are going to use our presampled datasets. The presampling is just to collect all point clouds, area by area and room by room, following PointGroup . mkdir -p data/S3DIS/ cd data/S3DIS gdown https://drive.google.com/uc?id = 1MX3ZCnwqyRztG1vFRiHkKTz68ZJeHS4Y tar -xvf s3disfull.tar Organize the dataset as follows: data |--- S3DIS |--- s3disfull |--- raw |--- Area_6_pantry_1.npy |--- ... |--- processed |--- s3dis_val_area5_0.040.pkl","title":"Dataset"},{"location":"examples/s3dis/#train","text":"For example, train PointNext-XL CUDA_VISIBLE_DEVICES = 1 python examples/segmentation/main.py cfgs/s3dis/pointnext-xl.yaml * change the cfg file to use any other model, e.g. cfgs/s3dis/pointnet++.yaml for training PointNet++ * run the command at the root directory","title":"Train"},{"location":"examples/s3dis/#test-on-area-5","text":"Note testing is a must step since evaluation in training is performed only on subsampled point clouds not original point clouds. CUDA_VISIBLE_DEVICES = 1 bash script/main_segmentation.sh cfgs/s3dis/pointnext-xl.yaml wandb.use_wandb = False mode = test --pretrained_path pretrained/s3dis/pointnext-xl/pointnext-xl-area5/checkpoint/pointnext-xl_ckpt_best.pth * add visualize=True to save segmentation results as .obj files","title":"Test on Area 5"},{"location":"examples/s3dis/#test-on-all-areas","text":"CUDA_VISIBLE_DEVICES = 1 python examples/segmentation/test_6fold.py cfgs/s3dis/pointnext-xl.yaml mode = test --pretrained_path pretrained/s3dis/pointnext-xl","title":"Test on All Areas"},{"location":"examples/s3dis/#profile-parameters-flops-and-throughput","text":"CUDA_VISIBLE_DEVICES = 1 python examples/profile.py --cfg cfgs/s3dis/pointnext-xl.yaml batch_size = 16 num_points = 15000 timing = True note: 1. set --cfg to cfgs/s3dis to profile all models under the folder. 2. you have to install the latest version of DeepSpeed from source to get a correct measurement of FLOPs","title":"Profile Parameters, FLOPs, and Throughput"},{"location":"examples/s3dis/#reference","text":"@inproceedings{armeni2016s3dis, title={3d semantic parsing of large-scale indoor spaces}, author={Armeni, Iro and Sener, Ozan and Zamir, Amir R and Jiang, Helen and Brilakis, Ioannis and Fischer, Martin and Savarese, Silvio}, booktitle=CVPR, pages={1534--1543}, year={2016} }","title":"Reference"},{"location":"examples/scannet/","text":"Large-Scale 3D Segmentation on ScanNet Dataset You can download our preprocessed ScanNet dataset as follows: cd data gdown https://drive.google.com/uc?id = 1uWlRPLXocqVbJxPvA2vcdQINaZzXf1z_ tar -xvf ScanNet.tar Please cite the ScanNet paper [1] if you are going to conduct experiments on it. Train For example, train PointNext-XL python examples/segmentation/main.py --cfg cfgs/scannet/pointnext-xl.yaml * change the cfg file to use any other model, e.g. cfgs/s3dis/pointnet++.yaml for training PointNet++ * run the command at the root directory Val python examples/segmentation/main.py --cfg cfgs/scannet/<YOUR_CONFIG> mode = test dataset.test.split = val --pretrained_path <YOUR_CHECKPOINT_PATH> Test You can generate Scannet benchmark submission file as follows CUDA_VISIBLE_DEVICES = 1 python examples/segmentation/main.py --cfg cfgs/scannet/<YOUR_CONFIG> mode = test dataset.test.split = test no_label = True pretrained_path = <YOUR_CHECKPOINT_PATH> Please make sure your checkpoint and your cfg matches with each. Reference @inproceedings{dai2017scannet, title={{ScanNet}: Richly-annotated {3D} Reconstructions of Indoor Scenes}, author={Dai, Angela and Chang, Angel X. and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Nie{\\ss}ner, Matthias}, booktitle = CVPR, year = {2017} }","title":"Large-Scale 3D Segmentation on ScanNet"},{"location":"examples/scannet/#large-scale-3d-segmentation-on-scannet","text":"","title":"Large-Scale 3D Segmentation on ScanNet"},{"location":"examples/scannet/#dataset","text":"You can download our preprocessed ScanNet dataset as follows: cd data gdown https://drive.google.com/uc?id = 1uWlRPLXocqVbJxPvA2vcdQINaZzXf1z_ tar -xvf ScanNet.tar Please cite the ScanNet paper [1] if you are going to conduct experiments on it.","title":"Dataset"},{"location":"examples/scannet/#train","text":"For example, train PointNext-XL python examples/segmentation/main.py --cfg cfgs/scannet/pointnext-xl.yaml * change the cfg file to use any other model, e.g. cfgs/s3dis/pointnet++.yaml for training PointNet++ * run the command at the root directory","title":"Train"},{"location":"examples/scannet/#val","text":"python examples/segmentation/main.py --cfg cfgs/scannet/<YOUR_CONFIG> mode = test dataset.test.split = val --pretrained_path <YOUR_CHECKPOINT_PATH>","title":"Val"},{"location":"examples/scannet/#test","text":"You can generate Scannet benchmark submission file as follows CUDA_VISIBLE_DEVICES = 1 python examples/segmentation/main.py --cfg cfgs/scannet/<YOUR_CONFIG> mode = test dataset.test.split = test no_label = True pretrained_path = <YOUR_CHECKPOINT_PATH> Please make sure your checkpoint and your cfg matches with each.","title":"Test"},{"location":"examples/scannet/#reference","text":"@inproceedings{dai2017scannet, title={{ScanNet}: Richly-annotated {3D} Reconstructions of Indoor Scenes}, author={Dai, Angela and Chang, Angel X. and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Nie{\\ss}ner, Matthias}, booktitle = CVPR, year = {2017} }","title":"Reference"},{"location":"examples/scanobjectnn/","text":"3D object classification on ScanObjectNN Dataset There are three ways to download the data: Download from the official website . Or, one can download the data by this command (please submit the \"ScanObjectNN Terms of Use\" form on their official website before downloading): mkdir -p data/ScanObjectNN cd data/ScanObjectNN wget http://hkust-vgd.ust.hk/scanobjectnn/h5_files.zip Or, one can only download the hardest variant by the following link. Please cite their paper[1] if you use the link to download the data mkdir data cd data gdown https://drive.google.com/uc?id = 1iM3mhMJ_N0x5pytcP831l3ZFwbLmbwzi tar -xvf ScanObjectNN.tar Organize the dataset as follows: data |--- ScanObjectNN |--- h5_files |--- main_split |--- training_objectdataset_augmentedrot_scale75.h5 |--- test_objectdataset_augmentedrot_scale75.h5 Train For example, train PointNext-S CUDA_VISIBLE_DEVICES = 1 python examples/classification/main.py --cfg cfgs/scanobjectnn/pointnext-s.yaml change the cfg file to use any other model, e.g. cfgs/scanobjectnn/pointnet++.yaml for training PointNet++ Test CUDA_VISIBLE_DEVICES = 1 python examples/classification/main.py --cfg cfgs/scanobjectnn/pointnext-s.yaml mode = test --pretrained_path pretrained/scanobjectnn/pointnext-s/pointnext-s_best.pth * change the cfg file to use any other model, e.g. cfgs/scanobjectnn/pointnet++.yaml for testing PointNet++ Profile parameters, FLOPs, and Throughput CUDA_VISIBLE_DEVICES=1 python examples/profile.py --cfg cfgs/scanobjectnn/pointnext-s.yaml batch_size=128 num_points=1024 timing=True note: 1. set --cfg to cfgs/scanobjectnn to profile all models under the folder. 2. you have to install the latest version of DeepSpeed from source to get a correct measurement of FLOPs Reference @inproceedings{uy-scanobjectnn-iccv19, title = {Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data}, author = {Mikaela Angelina Uy and Quang-Hieu Pham and Binh-Son Hua and Duc Thanh Nguyen and Sai-Kit Yeung}, booktitle = ICCV, year = {2019} }","title":"3D object classification on ScanObjectNN"},{"location":"examples/scanobjectnn/#3d-object-classification-on-scanobjectnn","text":"","title":"3D object classification on ScanObjectNN"},{"location":"examples/scanobjectnn/#dataset","text":"There are three ways to download the data: Download from the official website . Or, one can download the data by this command (please submit the \"ScanObjectNN Terms of Use\" form on their official website before downloading): mkdir -p data/ScanObjectNN cd data/ScanObjectNN wget http://hkust-vgd.ust.hk/scanobjectnn/h5_files.zip Or, one can only download the hardest variant by the following link. Please cite their paper[1] if you use the link to download the data mkdir data cd data gdown https://drive.google.com/uc?id = 1iM3mhMJ_N0x5pytcP831l3ZFwbLmbwzi tar -xvf ScanObjectNN.tar Organize the dataset as follows: data |--- ScanObjectNN |--- h5_files |--- main_split |--- training_objectdataset_augmentedrot_scale75.h5 |--- test_objectdataset_augmentedrot_scale75.h5","title":"Dataset"},{"location":"examples/scanobjectnn/#train","text":"For example, train PointNext-S CUDA_VISIBLE_DEVICES = 1 python examples/classification/main.py --cfg cfgs/scanobjectnn/pointnext-s.yaml change the cfg file to use any other model, e.g. cfgs/scanobjectnn/pointnet++.yaml for training PointNet++","title":"Train"},{"location":"examples/scanobjectnn/#test","text":"CUDA_VISIBLE_DEVICES = 1 python examples/classification/main.py --cfg cfgs/scanobjectnn/pointnext-s.yaml mode = test --pretrained_path pretrained/scanobjectnn/pointnext-s/pointnext-s_best.pth * change the cfg file to use any other model, e.g. cfgs/scanobjectnn/pointnet++.yaml for testing PointNet++","title":"Test"},{"location":"examples/scanobjectnn/#profile-parameters-flops-and-throughput","text":"CUDA_VISIBLE_DEVICES=1 python examples/profile.py --cfg cfgs/scanobjectnn/pointnext-s.yaml batch_size=128 num_points=1024 timing=True note: 1. set --cfg to cfgs/scanobjectnn to profile all models under the folder. 2. you have to install the latest version of DeepSpeed from source to get a correct measurement of FLOPs","title":"Profile parameters, FLOPs, and Throughput"},{"location":"examples/scanobjectnn/#reference","text":"@inproceedings{uy-scanobjectnn-iccv19, title = {Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data}, author = {Mikaela Angelina Uy and Quang-Hieu Pham and Binh-Son Hua and Duc Thanh Nguyen and Sai-Kit Yeung}, booktitle = ICCV, year = {2019} }","title":"Reference"},{"location":"examples/shapenetpart/","text":"Part Segmentation on ShapeNetPart Note in this experiment, we work on the ShapeNetPart Segmentation. The number of parts for each category is between 2 and 6, with 50 different parts in total. The data we use is: shapenetcore_partanno_segmentation_benchmark_v0_normal.zip [1]. We uniformly sample 2048 points in training and testing. Dataset Download the dataset from the official website, put the data under data/ShapeNetPart/ , and then run the training code once. The data will be autmmatically preprocessed (uniformly sample 2048 points). You can also use our preprocessed data provided below: cd data && mkdir ShapeNetPart && cd ShapeNetPart gdown https://drive.google.com/uc?id=1W3SEE-dY1sxvlECcOwWSDYemwHEUbJIS tar -xvf shapenetcore_partanno_segmentation_benchmark_v0_normal.tar Organize the dataset as follows: data |--- ShapeNetPart |--- shapenetcore_partanno_segmentation_benchmark_v0_normal |--- train_test_split |--- shuffled_train_file_list.json |--- ... |--- 02691156 |--- 1a04e3eab45ca15dd86060f189eb133.txt |--- ... |--- 02773838 |--- synsetoffset2category.txt |--- processed |--- trainval_2048_fps.pkl |--- test_2048_fps.pkl Train For example, train PointNeXt-S CUDA_VISIBLE_DEVICES = 0 python examples/shapenetpart/main.py --cfg cfgs/shapenetpart/pointnext-s.yaml - change cfg to cfgs/shapenetpart/pointnext-s_c160.yaml to train the best model we report in our paper. Test CUDA_VISIBLE_DEVICES = 0 python examples/shapenetpart/main.py cfgs/shapenetpart/pointnext-s.yaml mode = test --pretrained_path /path/to/your/pretrained_model Profile parameters, FLOPs, and Throughput CUDA_VISIBLE_DEVICES = 1 python examples/profile.py --cfg cfgs/shapenetpart/pointnext-s.yaml batch_size = 64 num_points = 2048 timing = True flops = True Reference @article{yi2016shapnetpart, Author = {Li Yi and Vladimir G. Kim and Duygu Ceylan and I-Chao Shen and Mengyan Yan and Hao Su and Cewu Lu and Qixing Huang and Alla Sheffer and Leonidas Guibas}, Journal = {SIGGRAPH Asia}, Title = {A Scalable Active Framework for Region Annotation in 3D Shape Collections}, Year = {2016}}`","title":"Part Segmentation on ShapeNetPart"},{"location":"examples/shapenetpart/#part-segmentation-on-shapenetpart","text":"Note in this experiment, we work on the ShapeNetPart Segmentation. The number of parts for each category is between 2 and 6, with 50 different parts in total. The data we use is: shapenetcore_partanno_segmentation_benchmark_v0_normal.zip [1]. We uniformly sample 2048 points in training and testing.","title":"Part Segmentation on ShapeNetPart"},{"location":"examples/shapenetpart/#dataset","text":"Download the dataset from the official website, put the data under data/ShapeNetPart/ , and then run the training code once. The data will be autmmatically preprocessed (uniformly sample 2048 points). You can also use our preprocessed data provided below: cd data && mkdir ShapeNetPart && cd ShapeNetPart gdown https://drive.google.com/uc?id=1W3SEE-dY1sxvlECcOwWSDYemwHEUbJIS tar -xvf shapenetcore_partanno_segmentation_benchmark_v0_normal.tar Organize the dataset as follows: data |--- ShapeNetPart |--- shapenetcore_partanno_segmentation_benchmark_v0_normal |--- train_test_split |--- shuffled_train_file_list.json |--- ... |--- 02691156 |--- 1a04e3eab45ca15dd86060f189eb133.txt |--- ... |--- 02773838 |--- synsetoffset2category.txt |--- processed |--- trainval_2048_fps.pkl |--- test_2048_fps.pkl","title":"Dataset"},{"location":"examples/shapenetpart/#train","text":"For example, train PointNeXt-S CUDA_VISIBLE_DEVICES = 0 python examples/shapenetpart/main.py --cfg cfgs/shapenetpart/pointnext-s.yaml - change cfg to cfgs/shapenetpart/pointnext-s_c160.yaml to train the best model we report in our paper.","title":"Train"},{"location":"examples/shapenetpart/#test","text":"CUDA_VISIBLE_DEVICES = 0 python examples/shapenetpart/main.py cfgs/shapenetpart/pointnext-s.yaml mode = test --pretrained_path /path/to/your/pretrained_model","title":"Test"},{"location":"examples/shapenetpart/#profile-parameters-flops-and-throughput","text":"CUDA_VISIBLE_DEVICES = 1 python examples/profile.py --cfg cfgs/shapenetpart/pointnext-s.yaml batch_size = 64 num_points = 2048 timing = True flops = True","title":"Profile parameters, FLOPs, and Throughput"},{"location":"examples/shapenetpart/#reference","text":"@article{yi2016shapnetpart, Author = {Li Yi and Vladimir G. Kim and Duygu Ceylan and I-Chao Shen and Mengyan Yan and Hao Su and Cewu Lu and Qixing Huang and Alla Sheffer and Leonidas Guibas}, Journal = {SIGGRAPH Asia}, Title = {A Scalable Active Framework for Region Annotation in 3D Shape Collections}, Year = {2016}}`","title":"Reference"},{"location":"projects/pix4point/","text":"Pix4Point: Image Pretrained Transformers for 3D Point Cloud Understanding by Guocheng Qian , Xingdi Zhang , Abdullah Hamdi , Bernard Ghanem arXiv | code News Sep, 2022: code released Abstract Pure Transformer models have achieved impressive success in natural language processing and computer vision. However, one limitation with Transformers is their need for large training data. In the realm of 3D point clouds, the availability of large datasets is a challenge, which exacerbates the issue of training Transformers for 3D tasks. In this work, we empirically study and investigate the effect of utilizing knowledge from a large number of images for point cloud understanding. We formulate a pipeline dubbed Pix4Point that allows harnessing pretrained Transformers in the image domain to improve downstream point cloud tasks. This is achieved by a modality-agnostic pure Transformer backbone with the help of tokenizer and decoder layers specialized in the 3D domain. Using image-pretrained Transformers, we observe significant performance gains of Pix4Point on the tasks of 3D point cloud classification, part segmentation, and semantic segmentation on ScanObjectNN, ShapeNetPart, and S3DIS benchmarks, respectively. Setup environment git clone this repository and install requirements: git clone git@github.com:guochengqian/Pix4Point.git cd Pix4point bash install.sh download the ImageNet21k pretrained Transformer, and put it in pretrained/imagenet/small_21k_224.pth gdown https://drive.google.com/file/d/1Iqc-nWVMmm4c8kYshNFcJsthnUy75Jl1/view?usp = sharing --fuzzy ImageNet Pretraining Please refer to DeiT's repo for details. S3DIS finetune Image Pretrained Transformer CUDA_VISIBLE_DEVICES = 0 python examples/segmentation/main.py --cfg cfgs/s3dis_sphere_pix4point/pix4point.yaml test CUDA_VISIBLE_DEVICES = 0 python examples/segmentation/main.py --cfg cfgs/s3dis_sphere_pix4point/pix4point.yaml mode = test pretrained_path = <pretrained_path> ## ScanObjectNN finetune CUDA_VISIBLE_DEVICES = 0 python examples/classification/main.py --cfg cfgs/scanobjectnn_pix4point/pvit.yaml ModelNet40 finetune CUDA_VISIBLE_DEVICES = 0 python examples/classification/main.py --cfg cfgs/modelnet40ply2048/pix4point.yaml ShapeNetPart finetune CUDA_VISIBLE_DEVICES = 0 python examples/shapenetpart/main.py --cfg cfgs/shapenetpart_pix4point/pix4point.yaml Citation If you are using our code in your work, please kindly cite the following: @inproceedings{qian2022pix4point, title={Pix4Point: Image Pretrained Transformers for 3D Point Cloud Understanding}, author={Guocheng Qian, Xingdi Zhang, Abdullah Hamdi, Bernard Ghanem}, publisher = {arXiv}, year={2022} }","title":"Pix4point"},{"location":"projects/pix4point/#pix4point-image-pretrained-transformers-for-3d-point-cloud-understanding","text":"by Guocheng Qian , Xingdi Zhang , Abdullah Hamdi , Bernard Ghanem","title":"Pix4Point: Image Pretrained Transformers for 3D Point Cloud Understanding"},{"location":"projects/pix4point/#arxiv-code","text":"","title":"arXiv | code"},{"location":"projects/pix4point/#news","text":"Sep, 2022: code released","title":"News"},{"location":"projects/pix4point/#abstract","text":"Pure Transformer models have achieved impressive success in natural language processing and computer vision. However, one limitation with Transformers is their need for large training data. In the realm of 3D point clouds, the availability of large datasets is a challenge, which exacerbates the issue of training Transformers for 3D tasks. In this work, we empirically study and investigate the effect of utilizing knowledge from a large number of images for point cloud understanding. We formulate a pipeline dubbed Pix4Point that allows harnessing pretrained Transformers in the image domain to improve downstream point cloud tasks. This is achieved by a modality-agnostic pure Transformer backbone with the help of tokenizer and decoder layers specialized in the 3D domain. Using image-pretrained Transformers, we observe significant performance gains of Pix4Point on the tasks of 3D point cloud classification, part segmentation, and semantic segmentation on ScanObjectNN, ShapeNetPart, and S3DIS benchmarks, respectively.","title":"Abstract"},{"location":"projects/pix4point/#setup-environment","text":"git clone this repository and install requirements: git clone git@github.com:guochengqian/Pix4Point.git cd Pix4point bash install.sh download the ImageNet21k pretrained Transformer, and put it in pretrained/imagenet/small_21k_224.pth gdown https://drive.google.com/file/d/1Iqc-nWVMmm4c8kYshNFcJsthnUy75Jl1/view?usp = sharing --fuzzy","title":"Setup environment"},{"location":"projects/pix4point/#imagenet-pretraining","text":"Please refer to DeiT's repo for details.","title":"ImageNet Pretraining"},{"location":"projects/pix4point/#s3dis","text":"finetune Image Pretrained Transformer CUDA_VISIBLE_DEVICES = 0 python examples/segmentation/main.py --cfg cfgs/s3dis_sphere_pix4point/pix4point.yaml test CUDA_VISIBLE_DEVICES = 0 python examples/segmentation/main.py --cfg cfgs/s3dis_sphere_pix4point/pix4point.yaml mode = test pretrained_path = <pretrained_path> ## ScanObjectNN finetune CUDA_VISIBLE_DEVICES = 0 python examples/classification/main.py --cfg cfgs/scanobjectnn_pix4point/pvit.yaml","title":"S3DIS"},{"location":"projects/pix4point/#modelnet40","text":"finetune CUDA_VISIBLE_DEVICES = 0 python examples/classification/main.py --cfg cfgs/modelnet40ply2048/pix4point.yaml","title":"ModelNet40"},{"location":"projects/pix4point/#shapenetpart","text":"finetune CUDA_VISIBLE_DEVICES = 0 python examples/shapenetpart/main.py --cfg cfgs/shapenetpart_pix4point/pix4point.yaml","title":"ShapeNetPart"},{"location":"projects/pix4point/#citation","text":"If you are using our code in your work, please kindly cite the following: @inproceedings{qian2022pix4point, title={Pix4Point: Image Pretrained Transformers for 3D Point Cloud Understanding}, author={Guocheng Qian, Xingdi Zhang, Abdullah Hamdi, Bernard Ghanem}, publisher = {arXiv}, year={2022} }","title":"Citation"},{"location":"projects/pointnext/","text":"PointNeXt: Revisiting PointNet++ with Improved Training and Scaling Strategies by Guocheng Qian , Yuchen Li , Houwen Peng , Jinjie Mai , Hasan Hammoud , Mohamed Elhoseiny , Bernard Ghanem arXiv | OpenPoints Library News Sep, 2022: PointNeXt accepted by NeurIPS'22 Abstract PointNet++ is one of the most influential neural architectures for point cloud understanding. Although the accuracy of PointNet++ has been largely surpassed by recent networks such as PointMLP and Point Transformer, we find that a large portion of the performance gain is due to improved training strategies, i.e. data augmentation and optimization techniques, and increased model sizes rather than architectural innovations. Thus, the full potential of PointNet++ has yet to be explored. In this work, we revisit the classical PointNet++ through a systematic study of model training and scaling strategies, and offer two major contributions. First, we propose a set of improved training strategies that significantly improve PointNet++ performance. For example, we show that, without any change in architecture, the overall accuracy (OA) of PointNet++ on ScanObjectNN object classification can be raised from 77.9\\% to 86.1%, even outperforming state-of-the-art PointMLP. Second, we introduce an inverted residual bottleneck design and separable MLPs into PointNet++ to enable efficient and effective model scaling and propose PointNeXt , the next version of PointNets. PointNeXt can be flexibly scaled up and outperforms state-of-the-art methods on both 3D classification and segmentation tasks. For classification, PointNeXt reaches an overall accuracy of 87.7\\% 87.7\\% on ScanObjectNN, surpassing PointMLP by 2.3\\% 2.3\\% , while being 10 \\times 10 \\times faster in inference. For semantic segmentation, PointNeXt establishes a new state-of-the-art performance with 74.9\\% 74.9\\% mean IoU on S3DIS (6-fold cross-validation), being superior to the recent Point Transformer. Visualization More examples are available in the paper . Citation If you find PointNeXt or the OpenPoints codebase useful, please cite: @Article { qian2022pointnext, author = { Qian, Guocheng and Li, Yuchen and Peng, Houwen and Mai, Jinjie and Hammoud, Hasan and Elhoseiny, Mohamed and Ghanem, Bernard } , journal = { arXiv:2206.04670 } , title = { PointNeXt: Revisiting PointNet++ with Improved Training and Scaling Strategies } , year = { 2022 } , }","title":"Pointnext"},{"location":"projects/pointnext/#pointnext-revisiting-pointnet-with-improved-training-and-scaling-strategies","text":"by Guocheng Qian , Yuchen Li , Houwen Peng , Jinjie Mai , Hasan Hammoud , Mohamed Elhoseiny , Bernard Ghanem","title":"PointNeXt: Revisiting PointNet++ with Improved Training and Scaling Strategies"},{"location":"projects/pointnext/#arxiv-openpoints-library","text":"","title":"arXiv | OpenPoints Library"},{"location":"projects/pointnext/#news","text":"Sep, 2022: PointNeXt accepted by NeurIPS'22","title":"News"},{"location":"projects/pointnext/#abstract","text":"PointNet++ is one of the most influential neural architectures for point cloud understanding. Although the accuracy of PointNet++ has been largely surpassed by recent networks such as PointMLP and Point Transformer, we find that a large portion of the performance gain is due to improved training strategies, i.e. data augmentation and optimization techniques, and increased model sizes rather than architectural innovations. Thus, the full potential of PointNet++ has yet to be explored. In this work, we revisit the classical PointNet++ through a systematic study of model training and scaling strategies, and offer two major contributions. First, we propose a set of improved training strategies that significantly improve PointNet++ performance. For example, we show that, without any change in architecture, the overall accuracy (OA) of PointNet++ on ScanObjectNN object classification can be raised from 77.9\\% to 86.1%, even outperforming state-of-the-art PointMLP. Second, we introduce an inverted residual bottleneck design and separable MLPs into PointNet++ to enable efficient and effective model scaling and propose PointNeXt , the next version of PointNets. PointNeXt can be flexibly scaled up and outperforms state-of-the-art methods on both 3D classification and segmentation tasks. For classification, PointNeXt reaches an overall accuracy of 87.7\\% 87.7\\% on ScanObjectNN, surpassing PointMLP by 2.3\\% 2.3\\% , while being 10 \\times 10 \\times faster in inference. For semantic segmentation, PointNeXt establishes a new state-of-the-art performance with 74.9\\% 74.9\\% mean IoU on S3DIS (6-fold cross-validation), being superior to the recent Point Transformer.","title":"Abstract"},{"location":"projects/pointnext/#visualization","text":"More examples are available in the paper .","title":"Visualization"},{"location":"projects/pointnext/#citation","text":"If you find PointNeXt or the OpenPoints codebase useful, please cite: @Article { qian2022pointnext, author = { Qian, Guocheng and Li, Yuchen and Peng, Houwen and Mai, Jinjie and Hammoud, Hasan and Elhoseiny, Mohamed and Ghanem, Bernard } , journal = { arXiv:2206.04670 } , title = { PointNeXt: Revisiting PointNet++ with Improved Training and Scaling Strategies } , year = { 2022 } , }","title":"Citation"}]}